{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    accuracy_score, \n",
    "    f1_score, \n",
    "    cohen_kappa_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Configuration\n",
    "# ==========================================\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 5e-4\n",
    "PATIENCE = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "print(\"--- Loading Data ---\")\n",
    "df_train = pd.read_csv(\"dataset/train_interpolated.csv\")\n",
    "df_valid = pd.read_csv(\"dataset/valid_interpolated.csv\")\n",
    "df_test  = pd.read_csv(\"dataset/test_interpolated.csv\")\n",
    "\n",
    "# 2. Apply Mapping and Drop Invalid Rows\n",
    "mapping_crop = {\n",
    "    27: \"Sesame\", 2: \"Pepper\", 8: \"Aralia\", 1: \"Sweet potato\",\n",
    "    17: \"Sudangrass\", 29: \"Soybean\", 9: \"Perilla\", 19: \"Greenhouse\",\n",
    "    24: \"Yuzu\", 23: \"Maize\", 28: \"Kiwi\", 22: \"Onion\",\n",
    "    16: \"Apple\", 30: \"Grape\", 14: \"Peach\", 10: \"Garlic\",\n",
    "    12: \"Pear\", 13: \"Cabbage\", 11: \"Sapling\", 31: \"Radish\"\n",
    "}\n",
    "\n",
    "for df in (df_train, df_valid, df_test):\n",
    "    df[\"crop_name\"] = df[\"CR_ID\"].map(mapping_crop)\n",
    "    df.dropna(subset=[\"crop_name\"], inplace=True)\n",
    "\n",
    "# 3. Define Features\n",
    "months = [f\"2021{m:02d}\" for m in range(7, 13)]\n",
    "bands = ['b02','b03','b04','b05','b06','b07','b08','b8a','b11','b12']\n",
    "features = [f\"{b}_{mon}_{d}\" for b in bands for mon in months for d in range(1, 4)]\n",
    "\n",
    "# 4. Prepare X and y\n",
    "le = LabelEncoder().fit(df_train[\"crop_name\"])\n",
    "\n",
    "X_train = df_train[features].values\n",
    "y_train = le.transform(df_train[\"crop_name\"])\n",
    "\n",
    "X_valid = df_valid[features].values\n",
    "y_valid = le.transform(df_valid[\"crop_name\"])\n",
    "\n",
    "X_test  = df_test[features].values\n",
    "y_test  = le.transform(df_test[\"crop_name\"])\n",
    "\n",
    "# 5. Scaling (MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# 6. Tensor Conversion & Reshaping\n",
    "# Structure: (Batch_Size, 1, Channels/Bands, Sequence_Length)\n",
    "# Assumes 18 steps (6 months * 3 intervals) and 12 bands\n",
    "n_steps = 18 \n",
    "n_features = X_train_scaled.shape[1]\n",
    "n_channels = n_features // n_steps # Should be 12\n",
    "\n",
    "print(f\"Feature Info: Channels={n_channels}, Steps={n_steps}, Total Features={n_features}\")\n",
    "\n",
    "def to_tensor_4d(x_scaled, n_channels, n_steps):\n",
    "    # Reshape to (N, 1, C, L) for Conv1d processing\n",
    "    return torch.tensor(x_scaled, dtype=torch.float32).reshape(-1, 1, n_channels, n_steps)\n",
    "\n",
    "X_train_t = to_tensor_4d(X_train_scaled, n_channels, n_steps)\n",
    "X_valid_t = to_tensor_4d(X_valid_scaled, n_channels, n_steps)\n",
    "X_test_t  = to_tensor_4d(X_test_scaled, n_channels, n_steps)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_valid_t = torch.tensor(y_valid, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 7. Create DataLoaders\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "valid_ds = TensorDataset(X_valid_t, y_valid_t)\n",
    "test_ds  = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(valid_ds, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"Data processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN+MLP\n",
    "\n",
    "class CNN_MLP(nn.Module):\n",
    "    def __init__(self, num_bands=12, seq_len=18, n_classes=20):\n",
    "        super(CNN_MLP, self).__init__()\n",
    "\n",
    "        # CNN Block: Temporal Convolution over the sequence length\n",
    "        # Input shape expected: (N, Bands, Seq_Len)\n",
    "        self.temporal_cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_bands, out_channels=64, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Classifier Block (MLP)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: (N, 1, Bands, Seq_Len)\n",
    "        x = x.squeeze(1)            # Remove the extra dimension -> (N, Bands, Seq_Len)\n",
    "        x = self.temporal_cnn(x)    # -> (N, 256, Seq_Len)\n",
    "        x = self.global_pool(x)     # -> (N, 256, 1)\n",
    "        x = x.squeeze(-1)           # -> (N, 256)\n",
    "        out = self.classifier(x)    # -> (N, n_classes)\n",
    "        return out\n",
    "\n",
    "# Initialize Model\n",
    "model = CNN_MLP(\n",
    "    num_bands=n_channels, \n",
    "    seq_len=n_steps, \n",
    "    n_classes=len(le.classes_)\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN+MLP TRAIN\n",
    "#  1. Setup Loss, Optimizer, Scheduler\n",
    "class_weight_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weight_array, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(1, NUM_EPOCHS // 10), gamma=0.8)\n",
    "\n",
    "# 2. Training Loop with Early Stopping\n",
    "best_val_loss = np.inf\n",
    "patience_cnt = 0\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Train Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Calculate average loss\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss   /= len(val_loader.dataset)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"[{epoch:3d}/{NUM_EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6g}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        if patience_cnt >= PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN+MLP TEST\n",
    "# 1. Load Best Model\n",
    "print(\"--- Loading Best Model for Evaluation ---\")\n",
    "model.load_state_dict(torch.load(\"weights/CNN+MLP.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Calculate Parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {n_params}\")\n",
    "\n",
    "# 2. Measure Inference Time & Predict\n",
    "all_preds, all_labels = [], []\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 3. Performance Metrics\n",
    "total_time = end_time - start_time\n",
    "inference_time_per_sample = (total_time / len(all_labels)) * 1000\n",
    "\n",
    "print(f\"\\nInference Time: {inference_time_per_sample:.4f} ms/sample\")\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(all_labels, all_preds, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBAM+MLP\n",
    "class CBAM1D(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super(CBAM1D, self).__init__()\n",
    "        \n",
    "        # Channel Attention Module\n",
    "        self.channel_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.channel_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.channel_mlp = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels)\n",
    "        )\n",
    "\n",
    "        # Spatial Attention Module\n",
    "        # Using kernel_size=7 as default for larger receptive field in 1D\n",
    "        self.spatial_conv = nn.Conv1d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ----- Channel Attention -----\n",
    "        # Input: (N, C, L)\n",
    "        avg_pool = self.channel_avg_pool(x).squeeze(-1)  # (N, C)\n",
    "        max_pool = self.channel_max_pool(x).squeeze(-1)  # (N, C)\n",
    "        \n",
    "        channel_attn = self.channel_mlp(avg_pool) + self.channel_mlp(max_pool)\n",
    "        channel_attn = torch.sigmoid(channel_attn).unsqueeze(-1)  # (N, C, 1)\n",
    "        \n",
    "        x = x * channel_attn # Broadcast multiplication\n",
    "\n",
    "        # ----- Spatial Attention -----\n",
    "        # Compress channel dimension using Mean and Max\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)    # (N, 1, L)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # (N, 1, L)\n",
    "        \n",
    "        spatial_attn = torch.cat([avg_out, max_out], dim=1)       # (N, 2, L)\n",
    "        spatial_attn = torch.sigmoid(self.spatial_conv(spatial_attn)) # (N, 1, L)\n",
    "        \n",
    "        x = x * spatial_attn\n",
    "        return x\n",
    "\n",
    "# 2. Main Classifier with CBAM\n",
    "class CBAM_MLP(nn.Module):\n",
    "    def __init__(self, num_bands=12, seq_len=18, n_classes=20):\n",
    "        super(CBAM_MLP, self).__init__()\n",
    "\n",
    "        # Temporal Feature Extraction\n",
    "        self.temporal_cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_bands, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(64, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Attention Mechanism\n",
    "        self.cbam = CBAM1D(channels=256, reduction=16, kernel_size=7)\n",
    "\n",
    "        # Global Pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Classification Head (MLP)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (N, 1, Bands, Seq_Len)\n",
    "        x = x.squeeze(1)            # (N, Bands, Seq_Len)\n",
    "        x = self.temporal_cnn(x)    # (N, 256, Seq_Len)\n",
    "        x = self.cbam(x)            # (N, 256, Seq_Len) - Refined features\n",
    "        x = self.global_pool(x)     # (N, 256, 1)\n",
    "        x = x.squeeze(-1)           # (N, 256)\n",
    "        out = self.classifier(x)    # (N, n_classes)\n",
    "        return out\n",
    "\n",
    "# Initialize Model\n",
    "model_cbam = CBAM_MLP(\n",
    "    num_bands=n_channels, \n",
    "    seq_len=n_steps, \n",
    "    n_classes=len(le.classes_)\n",
    ").to(DEVICE)\n",
    "\n",
    "print(\"CBAM Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM+MLP TRAIN\n",
    "#  1. Setup Loss, Optimizer, Scheduler\n",
    "class_weight_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weight_array, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(1, NUM_EPOCHS // 10), gamma=0.8)\n",
    "\n",
    "# 2. Training Loop with Early Stopping\n",
    "best_val_loss = np.inf\n",
    "patience_cnt = 0\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Train Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Calculate average loss\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss   /= len(val_loader.dataset)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"[{epoch:3d}/{NUM_EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6g}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"CBAM+MLP.pt\")\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        if patience_cnt >= PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM+MLP TEST\n",
    "# 1. Load Best Model\n",
    "print(\"--- Loading Best Model for Evaluation ---\")\n",
    "model.load_state_dict(torch.load(\"weights/CBAM+MLP.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Calculate Parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {n_params}\")\n",
    "\n",
    "# 2. Measure Inference Time & Predict\n",
    "all_preds, all_labels = [], []\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 3. Performance Metrics\n",
    "total_time = end_time - start_time\n",
    "inference_time_per_sample = (total_time / len(all_labels)) * 1000\n",
    "\n",
    "print(f\"\\nInference Time: {inference_time_per_sample:.4f} ms/sample\")\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(all_labels, all_preds, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN+TF\n",
    "class CNN_TF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_bands=10,\n",
    "        seq_len=18,\n",
    "        n_classes=21,\n",
    "        embed_dim=512,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        cnn_dropout=0.1,\n",
    "        cls_dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- 1) Temporal CNN + 1D-Dropout + Pooling (18 -> 9) ---\n",
    "        self.temporal_cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_bands, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout1d(cnn_dropout),\n",
    "\n",
    "            nn.Conv1d(64, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout1d(cnn_dropout),\n",
    "\n",
    "            nn.Conv1d(256, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(embed_dim), nn.ReLU(), nn.Dropout1d(cnn_dropout),\n",
    "\n",
    "            nn.MaxPool1d(kernel_size=2)  # seq_len 18 -> 9\n",
    "        )\n",
    "        reduced_len = seq_len // 2  # 9\n",
    "\n",
    "        # --- 2) Learnable CLS token + Positional Embedding ---\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_emb   = nn.Parameter(torch.randn(1, reduced_len + 1, embed_dim))\n",
    "\n",
    "        # --- 3) Transformer Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # --- 4) Classifier on CLS token ---\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cls_dropout),\n",
    "            nn.Linear(128, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, num_bands, seq_len)\n",
    "        x = x.squeeze(1)             # -> (B, num_bands, seq_len)\n",
    "        x = self.temporal_cnn(x)     # -> (B, embed_dim, reduced_len)\n",
    "        x = x.permute(0, 2, 1)       # -> (B, reduced_len, embed_dim)\n",
    "\n",
    "        B, T, D = x.shape\n",
    "        cls = self.cls_token.expand(B, -1, -1)         # -> (B, 1, D)\n",
    "        x   = torch.cat([cls, x], dim=1)               # -> (B, T+1, D)\n",
    "        x   = x + self.pos_emb                         # add positional info\n",
    "\n",
    "        x = self.transformer(x)                       # -> (B, T+1, D)\n",
    "        cls_feat = x[:, 0, :]                         # take CLS\n",
    "\n",
    "        out = self.classifier(cls_feat)               # -> (B, n_classes)\n",
    "        return out\n",
    "    \n",
    "model = CNN_TF(\n",
    "    num_bands=10,\n",
    "    seq_len=n_steps,\n",
    "    n_classes=len(le.classes_),\n",
    "    embed_dim=512,\n",
    "    num_heads=4,\n",
    "    num_layers=2\n",
    "\n",
    ").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN+TF TRAIN\n",
    "#  1. Setup Loss, Optimizer, Scheduler\n",
    "class_weight_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weight_array, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(1, NUM_EPOCHS // 10), gamma=0.8)\n",
    "\n",
    "# 2. Training Loop with Early Stopping\n",
    "best_val_loss = np.inf\n",
    "patience_cnt = 0\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Train Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Calculate average loss\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss   /= len(val_loader.dataset)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"[{epoch:3d}/{NUM_EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6g}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"CNN+TF.pt\")\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        if patience_cnt >= PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN+TF TEST\n",
    "# 1. Load Best Model\n",
    "print(\"--- Loading Best Model for Evaluation ---\")\n",
    "model.load_state_dict(torch.load(\"weights/CNN+TF.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Calculate Parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {n_params}\")\n",
    "\n",
    "# 2. Measure Inference Time & Predict\n",
    "all_preds, all_labels = [], []\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 3. Performance Metrics\n",
    "total_time = end_time - start_time\n",
    "inference_time_per_sample = (total_time / len(all_labels)) * 1000\n",
    "\n",
    "print(f\"\\nInference Time: {inference_time_per_sample:.4f} ms/sample\")\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(all_labels, all_preds, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM+TF\n",
    "\n",
    "class CBAMChannel1D(nn.Module):\n",
    "    def __init__(self, channels:int, reduction:int=16, use_max:bool=True, init_alpha:float=0.5):\n",
    "        super().__init__()\n",
    "        hidden = max(1, channels // reduction)  \n",
    "        self.use_max = use_max\n",
    "\n",
    "        # Squeeze: (B, C, 1) → (B, C)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        if use_max:\n",
    "            self.max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        # Shared MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(channels, hidden, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, channels, bias=True)\n",
    "        )\n",
    "\n",
    "        # learnable gate\n",
    "        self.alpha = nn.Parameter(torch.tensor(init_alpha, dtype=torch.float32))\n",
    "\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):  # x: (B, C, T)\n",
    "        avg = self.avg_pool(x).squeeze(-1)       # (B, C)\n",
    "        s_avg = self.mlp(avg)                    # (B, C)\n",
    "\n",
    "        if self.use_max:\n",
    "            mx = self.max_pool(x).squeeze(-1)    # (B, C)\n",
    "            s_max = self.mlp(mx)                 # (B, C)\n",
    "            s = s_avg + s_max\n",
    "        else:\n",
    "            s = s_avg\n",
    "\n",
    "        scale = torch.sigmoid(s).unsqueeze(-1)   # (B, C, 1)\n",
    "\n",
    "        # Residual gating\n",
    "        y = x * (1.0 + self.alpha * scale)\n",
    "        return y\n",
    "\n",
    "    \n",
    "class TempCNNTransformerCropClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_bands=10,\n",
    "        seq_len=18,\n",
    "        n_classes=10,\n",
    "        embed_dim=256,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "        cnn_dropout=0.2,\n",
    "        cls_dropout=0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ----- CNN -----\n",
    "        self.temporal_cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=num_bands, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(64, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv1d(256, embed_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # ----- CBAM + Dropout -----\n",
    "        self.cbam = CBAMChannel1D(embed_dim, reduction=16, use_max=True, init_alpha=0.5)\n",
    "        self.cnn_dropout = nn.Dropout(cnn_dropout)\n",
    "\n",
    "        # ----- Pooling 후 Transformer 입력 길이 줄이기 -----\n",
    "        self.pool = nn.AdaptiveAvgPool1d(9)  # → 시퀀스 길이 9으로 압축\n",
    "        reduced_len = 9\n",
    "\n",
    "        # ----- CLS token + Positional Embedding -----\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))  # (1, 1, 256)\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, reduced_len + 1, embed_dim))  # (1, 10, 256)\n",
    "\n",
    "        # ----- Transformer Encoder -----\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.2,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # ----- Classifier -----\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cls_dropout),\n",
    "            nn.Linear(64, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, num_bands, seq_len)\n",
    "        x = x.squeeze(1)                        # → (B, num_bands, seq_len)\n",
    "        x = self.temporal_cnn(x)               # → (B, embed_dim, seq_len)\n",
    "        x = self.cbam(x)                        # → (B, embed_dim, seq_len)\n",
    "        x = self.cnn_dropout(x)\n",
    "        x = self.pool(x)                        # → (B, embed_dim, 9)\n",
    "        x = x.permute(0, 2, 1)                  # → (B, 9, embed_dim)\n",
    "\n",
    "        # CLS token 추가\n",
    "        B = x.size(0)\n",
    "        cls = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat([cls, x], dim=1)          # (B, 10, embed_dim)\n",
    "        x = x + self.pos_emb                    # Positional Encoding 추가\n",
    "\n",
    "        x = self.transformer(x)                 # → (B, 10, embed_dim)\n",
    "        cls_feat = x[:, 0, :]                   # → (B, embed_dim)\n",
    "        return self.classifier(cls_feat)        # → (B, n_classes)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "model = TempCNNTransformerCropClassifier(\n",
    "    num_bands=10,\n",
    "    seq_len=n_steps,\n",
    "    n_classes=n_classes,\n",
    "    embed_dim=512,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    cnn_dropout=0.2,\n",
    "    cls_dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weight_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weight_array, dtype=torch.float32, device=device)\n",
    "\n",
    "# Criterion, Optimizer, Scheduler\n",
    "criterion  = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(1, 10), gamma=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM+TF TRAIN\n",
    "#  1. Setup Loss, Optimizer, Scheduler\n",
    "class_weight_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weight_array, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(1, NUM_EPOCHS // 10), gamma=0.8)\n",
    "\n",
    "# 2. Training Loop with Early Stopping\n",
    "best_val_loss = np.inf\n",
    "patience_cnt = 0\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Train Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Calculate average loss\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss   /= len(val_loader.dataset)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"[{epoch:3d}/{NUM_EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6g}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"CBAM+TF.pt\")\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        if patience_cnt >= PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM+TF TEST\n",
    "# 1. Load Best Model\n",
    "print(\"--- Loading Best Model for Evaluation ---\")\n",
    "model.load_state_dict(torch.load(\"weights/CBAM+TF.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Calculate Parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {n_params}\")\n",
    "\n",
    "# 2. Measure Inference Time & Predict\n",
    "all_preds, all_labels = [], []\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 3. Performance Metrics\n",
    "total_time = end_time - start_time\n",
    "inference_time_per_sample = (total_time / len(all_labels)) * 1000\n",
    "\n",
    "print(f\"\\nInference Time: {inference_time_per_sample:.4f} ms/sample\")\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(all_labels, all_preds, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1,  64, 3, padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128,128,3,padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            # Block 3 \n",
    "            nn.Conv2d(128,256,3,padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256,3,padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),  \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = CNN(n_classes=len(le.classes_)).to(DEVICE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN TRAIN\n",
    "#  1. Setup Loss, Optimizer, Scheduler\n",
    "class_weight_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weight_array, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(1, NUM_EPOCHS // 10), gamma=0.8)\n",
    "\n",
    "# 2. Training Loop with Early Stopping\n",
    "best_val_loss = np.inf\n",
    "patience_cnt = 0\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Train Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Calculate average loss\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss   /= len(val_loader.dataset)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"[{epoch:3d}/{NUM_EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6g}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"CNN.pt\")\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        if patience_cnt >= PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN TEST\n",
    "# 1. Load Best Model\n",
    "print(\"--- Loading Best Model for Evaluation ---\")\n",
    "model.load_state_dict(torch.load(\"weights/CNN.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Calculate Parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {n_params}\")\n",
    "\n",
    "# 2. Measure Inference Time & Predict\n",
    "all_preds, all_labels = [], []\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 3. Performance Metrics\n",
    "total_time = end_time - start_time\n",
    "inference_time_per_sample = (total_time / len(all_labels)) * 1000\n",
    "\n",
    "print(f\"\\nInference Time: {inference_time_per_sample:.4f} ms/sample\")\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(all_labels, all_preds, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_planes, in_planes // ratio),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_planes // ratio, in_planes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out).view(b, c, 1, 1)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x))\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, ratio=16):\n",
    "        super().__init__()\n",
    "        self.ca = ChannelAttention(channels, ratio)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.ca(x)\n",
    "        x = x * self.sa(x)\n",
    "        return x\n",
    "\n",
    "# ================================================================\n",
    "# 2) Residual + CBAM \n",
    "# ================================================================\n",
    "class CNN_CBAM(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.cbam1 = CBAM(64)\n",
    "        self.pool1 = nn.Sequential(\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.cbam2 = CBAM(64)\n",
    "        self.pool2 = nn.Sequential(\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.cbam3 = CBAM(256)\n",
    "        self.pool3 = nn.Sequential(\n",
    "            nn.MaxPool2d((2, 1)),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.cbam1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.cbam2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.cbam3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ================================================================\n",
    "# 4) 사용 예시\n",
    "# ================================================================\n",
    "model = CNN_CBAM(n_classes=len(le.classes_)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM TRAIN\n",
    "#  1. Setup Loss, Optimizer, Scheduler\n",
    "class_weight_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weight_array, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=5e-3)\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, step_size=max(1, NUM_EPOCHS // 10), gamma=0.8)\n",
    "\n",
    "# 2. Training Loop with Early Stopping\n",
    "best_val_loss = np.inf\n",
    "patience_cnt = 0\n",
    "\n",
    "print(\"--- Starting Training ---\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # Train Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # Calculate average loss\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss   /= len(val_loader.dataset)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"[{epoch:3d}/{NUM_EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6g}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"CBAM.pt\")\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        if patience_cnt >= PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM TEST\n",
    "# 1. Load Best Model\n",
    "print(\"--- Loading Best Model for Evaluation ---\")\n",
    "model.load_state_dict(torch.load(\"weights/CBAM.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Calculate Parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total Parameters: {n_params}\")\n",
    "\n",
    "# 2. Measure Inference Time & Predict\n",
    "all_preds, all_labels = [], []\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# 3. Performance Metrics\n",
    "total_time = end_time - start_time\n",
    "inference_time_per_sample = (total_time / len(all_labels)) * 1000\n",
    "\n",
    "print(f\"\\nInference Time: {inference_time_per_sample:.4f} ms/sample\")\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "kappa = cohen_kappa_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(all_labels, all_preds, target_names=le.classes_, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
